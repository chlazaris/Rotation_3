### Date: April 14th 2014
### Title: Creating a script to convert Ren data (from Nature 2013 paper) to the appropriate format to be used by genomic_tools 

<!-- What to do -->

#### What to do

1. Install genomic_tools on the cluster
2. Write a script to turn .txt Hi-C files to reg.gz files
3. Think about the next steps for producing matrices and
heatmaps using the Ren data.

<!-- **Bold text**  *Emphasized text* -->

<!-- Non-enumerated list -->

* I download the source code for genomic_tools from 
`https://code.google.com/p/ibm-cbc-genomic-tools/source/checkout`  and then I installed it locally based on the instructions. I also
set up the $PATH to be able to call any of the commands of the package from anywhere.
The version installed is 2.8.0 which includes the code for hic analysis. Type
`gtools_hic -h` any time help for the possible options is required.

* I wrote a script named `txt_to_reg.sh` which reads the .txt files in the current directory and converts them to the .reg format. The script is placed in the **Scripts** directory in my home directory on the cluster.

* In order to perform the binning (split each chromosome in 1MB chunks)
  I will use genomic_tools. The syntax is shown below (with the first of the .gz files as input):
  
	`gtools_hic bin --bin-size 1000000 -v reg.gz`
  
	where:
  
	- gtools_hic bin is the command for the binning
	-  --bin-size: is the size of each bin (default 1MB)
	- -v: verbose

* Caution: Use *gunzip* when you want to unzip something and *gzip* when you want to compress it.

Moreover, from now on I will be writing the lab-book in Markdown as it is much easier than LaTeX in terms of placing URLs and code in-line with the text. 

For this reason I added the current document to my private repository on GitHub and this is the document I will be modifying every day.

<!--End of the day-->

<!-- Start day -->

### Date: April 16th 2014
### Title: Create matrices of different resolutions using Ren's Nature 2013 data
### and also test the Python *hic_to_matrix* script when using the right input
### (file that corresponds to bins when having 100MB resolution). 

#### What to do 

1. Write a bash script to automate matrix creation based on Aris' gtools
2. Check your Python script with the right input to see if it works. Change
it to handle bed files properly if you can.
3. You have to start working on the table reviewing all Hi-C papers as well.

<!-- **Bold text**  *Emphasized text* -->

<!-- Non-enumerated list -->

* I used `gtools_hic bin --bin-size 100000000 -v GSM1055800_HiC.IMR90.rep1.nodup.summary.reg.gz | head > GSM1055800_HiC.IMR90.rep1.nodup.summary.head.txt` in order to create
that dummy file that I will used as input to my script. The resolution in this
case is correct to compare with Aris .dat file (100MB)
* Indeed, I used the command `cat GSM1055800_HiC.IMR90.rep1.nodup.summary.reg.gz | gunzip | head -10 | gtools_hic bin -v --bin-size 100000000 > 100MB_input` to produce the correct input to use on my script. It is worth mentioning here that head somehow produces only the 5 first lines! When I compared the 100MB_input.dat (matrix generated using my script) with out\_100MB.dat (matrix generated using Aris' method) they were identical (see diff below)!
* As Aris said, I generated the matrices using his method for the following
resolutions 500KB, 1MB, 5MB, 100MB (I also did 256KB, 1024KB, 4096KB to be able to compare
with previous boxplots). All data and the results of this analysis can be found on cluster in
`Ren-Nature2013a`.  


# Date: April 17th 2014
## Title: Improving the hic\_to\_matrix.py and timing it. 

<!-- What to do -->

1. Add functionality to hic\_to\_matrix.py
2. Time it vs. Aris' C++ implementation


<!-- **Bold text**  *Emphasized text* -->

<!-- Non-enumerated list -->

* I made it possible to read standard input
and get rif of the intermediate file that I was
producing. So, now the script does not depend at
all on Aris' pipeline
* I performed a timing of the execution of my script
vs. Aris' implementation
* The results can be found in the `time_hic_to_matrix`
directory in `Ren_Nature_2013a` on the cluster. 

# Date: April 21st 2014
## Title: Checking Aris' method when using 50kb resulution.

<!-- What to do -->

1. When Aris' method was used with 1024kb and 4096kb resolution, the correlations we get are extremely high
no matter how different the samples were. So, we decided
to try with 50kb resolution.
2.
3.

<!-- **Bold text**  *Emphasized text* -->

<!-- Non-enumerated list -->

* I created the 50k resolution matrix corresponding to the first sample using the command given below:
`cat GSM1055800_HiC.IMR90.rep1.nodup.summary.reg.gz | gunzip | gtools_hic bin -v --bin-size 50000 --matrix > GSM1055800_HiC.IMR90.rep1.nodup.summary.50000.dat`

*SOS* This is wrong, I had to include the genome as well. So,
the correct command is:

`cat GSM1055800_HiC.IMR90.rep1.nodup.summary.reg.gz | gunzip | gtools_hic bin -v --bin-size 50000 -g hg19.genome.bed --matrix > GSM1055800_HiC.IMR90.rep1.nodup.summary.50000.dat`


* Then in order to create the corresponding .Rdata file:
`Rscript hic_matrix.r estimate -v --algorithm fused1d_flsa --gamma 2 --lambda 2 -o GSM1055800_HiC.IMR90.rep1.nodup.summary.50000.RData -C hg19.w=50000.bed GSM1055800_HiC.IMR90.rep1.nodup.summary.50000.dat`

*Caution* The .bed matrix can be used directly. There is no need to isolate the first line and create the corresponding vector.

**Problems**

* For some reason the matrices are not created corrected
while I have the option matrix there.

OK, this problem is now solved. I had not put the genome file
(the option -g hg19.genome.bed).

* Is the hic_matrix.r file I am using good enough to handle the extra (1D) option? Yes, because Aris updated the file today.

<!-- Code goes here -->
<!-- ``
 -->
<!-- Link goes here -->
<!-- This is [an example](http://example.com/ "Title") inline link.
 -->

 # Date: 24-25 April 2014
 ## Title: Running Case 1 - Lasso without sparsity: Three resolutions
 ## (10kb, 100kb, 1MB) for Dekker-2009 data and Ren-2012 (topological
 ## domains) data.
 
 <!-- What I did -->
 
 1. Run `create_chrom_matrices` script that Aris wrote in order to do the 
 	binning (10kb, 100kb, 1MB). For this purpose I wrote the scripts `run_chrom_matrices_Ren.sh` and `run_chrom_matrices_Dekker.sh`.
 	These scripts take as input the name of the restriction enzyme used
 	for Hi--C and the resolution that we want for binning (in bp).
    They are run from the directory of the resolution and as output they
    have new directories where all the chromosome matrices will be placed.


 2. Run `hic_matrix.r` script that Aris wrote to do the estimation. For this 
 	purpose, I wrote the `run_hic_matrix.sh` script. The script requires
 	as argument the name of the directory where the matrix files are placed.
 	The result of running this will be to create new files with the name 
 	`estimated.chr_name.dat` files in the same directory where the matrix
 	files are.
 
 3. In this third step the comparison is performed using `hic_matrix.r compare`
    (option of teh .R script Aris wrote). For this purpose, I wrote a bash 
    script called `compare_RData.sh` which accepts as arguments the names of the
    two directories that contain the files I want to perform the comparison with and
    as third argument F which is a number from 0 to 1 corresponding to the fraction 
    of the image we want to output (0 for no image, 1 for the whole chromosome).
    The output is a directory with all the comparisons eg. `compare.hg19.GM-HindIII-HiC.vs.hg19.GM-NcoI-HiC`. The comparisons are .pdf files (at the moment 2 page-long files) with gamma vs lambda plots and the corresponding Pearson and Spearman coefficients as well as heatmaps (on second page).

    The bash scripts I wrote will be put in my personal private repository on GitHub for safety.
 
 **Future directions**  
 
 <!-- Non-enumerated list -->
 
 * Complete the same analysis
   with 10kb resolution and again 
   for chromosome one
 * Do the binning for the most recent
   Ren data and start performing the
   same analysis.

<!-- BEGIN OF DAY -->

### Date: May 7th 2014
#### Title: Running experiments with gamma=0 and resolution 100kb
#### only for chromosome 1.
 
 <!-- What to do -->
 
 1. Focus on chr1 and resolution 100kb
 2. Run without preprocessing (preprocess.none)
 3. Use only gamma=0 and run HindIII.vs.NcoI, HindIII.random.vs.NcoI.random
 and HindIII.complete.random.vs.NcoI.complete.random
 
 The results of these analysis are in the directory `Evaluation/Case1/Comparisons/res100kb.chr1.gamma0`. The directory `chrom_by_chrom` contains
 the results of the analysis for two different preprocessing methods (log2 and 
 'none') as well as for two resolutions (100kb and 1MB). 

 In order to perform the analysis for the 100kb resolution and gamma 0 for
 chromosome 1, I followed the following steps:
 
 <!-- Non-enumerated list -->
 
 * Initially I changed the value of min-gamma, max-gamma and n-gamma in Aris'
 `hic_matrix.r` script but then I decided to leave the script intact and
 change my `run_hic_matrix` script which as it stands right now, it requires
 three different parameters:

 - The input directory (which is the directory that contains the matrix file
 where the estimation will be performed)

 - The chromosome (chr1) which is the chrom for which the estimation will
 be performed

 - The preprocessing type. In this case it is `none`

 - The algorithm used. I was asked to use `fused1Dalt_flsa` instead of `fused1D_flsa`
 
 <!-- Code goes here -->
Jobs that I submitted:

 `qsub /ifs/home/cl3011/ROTATION_3/Evaluation/Case1/Scripts/run_hic_matrix.sh mm10.mESC_J1-HindIII-HiC chr1 none fused1Dalt_flsa`

 `qsub /ifs/home/cl3011/ROTATION_3/Evaluation/Case1/Scripts/run_hic_matrix.sh mm10.mESC_J1-HindIII-HiC-random chr1 none fused1Dalt_flsa`

 `qsub /ifs/home/cl3011/ROTATION_3/Evaluation/Case1/Scripts/run_hic_matrix.sh mm10.mESC_J1-HindIII-HiC-random-complete chr1 none fused1Dalt_flsa`

 and the corresponding ones for NcoI.

**Future directions**  
 
* On the resulting files, run `compare.RData` to get the corresponding
correlations and heatmaps (DO NOT FORGET TO PUT -f 1)
* Perform the following comparisons:
  1. HindIII vs. NcoI
  2. HindIII.random vs. NcoI.random
  3. HindIII.complete.random vs. NcoI.complete.random

<!-- END OF DAY -->
 
<!-- BEGIN DAY -->

### Date: May 8th 2014
#### Title: Running lasso (using the fused1Dalt algorithm) with lambda between 0 and 10000 (n=100) and using logarithmic scale for lambda.

**Do not forget to change the scale back to normal if you do not want logarithmic anymore**

<!-- What to do -->

I changed the `run_hic_matrix.sh` accordingly in order to be able to define the necessary parameters. As it is, I can specify the target directory, the preprocess, the value of maximum lambda, the number of different lambda (n) and the algorithm that is used (fused1Dalt_flsa)
in this case. For today I also added the option for plotting lambda in logarithmic scale. The commands I ran are given below, along with the corresponding timestamps.

`826  08/05/14 13:25:57 qsub /ifs/home/cl3011/ROTATION_3/Evaluation/Case1/Scripts/run_hic_matrix.sh mm10.mESC_J1-HindIII-HiC chr1 none 10000 100 fused1Dalt_flsa`

`827  08/05/14 13:26:22 qsub /ifs/home/cl3011/ROTATION_3/Evaluation/Case1/Scripts/run_hic_matrix.sh mm10.mESC_J1-NcoI-HiC chr1 none 10000 100 fused1Dalt_flsa`

` 828  08/05/14 13:26:44 qsub /ifs/home/cl3011/ROTATION_3/Evaluation/Case1/Scripts/run_hic_matrix.sh mm10.mESC_J1-HindIII-HiC-random chr1 none 10000 100 fused1Dalt_flsa`

`829  08/05/14 13:26:57 qsub /ifs/home/cl3011/ROTATION_3/Evaluation/Case1/Scripts/run_hic_matrix.sh mm10.mESC_J1-NcoI-HiC-random chr1 none 10000 100 fused1Dalt_flsa`
<!-- Non-enumerated list -->

I also successfully wrote a matlab function (placed in `random_matrix_matlab` directory on desktop). While the function
definitely works correctly, the unshuffled matrix and the resulting
random matrix (after running the script) have different sizes. I have to discuss it with Aris and find out what is going wrong (if any).

The results of today's analysis can be found in:
`/ifs/home/cl3011/ROTATION_3/Evaluation/Case1/Comparisons/preprocess.none/resolution.100kb/gamma0/log2.lambda/lambda_max_10000/n_lambda_100/chr1`

<!-- END OF DAY -->
 

# Date: May 21 2014
## Title: Summary of the work done on Panos Hi-C data on CUTLL in the last 2 weeks

<!-- What to do -->

1. Matrices were created from filtered reads using `create_chrom_matrices`. We are
interested in two resolutions (100kb and 1MB).

The datasets used for the creation of the matrices (CUTLL.control.GSI and CUTLL.HiC)
were the following:

**Give me back the files that much the pattern (these are the GSI ones)**
X1=$(ls -1 `Panos_data/run-*/analysis/hiclib/hiclib.CUTLL1-*/filtered_reads.reg+` | grep "GSI.-rep.-HiC/filtered_reads.reg+$")

echo $X1

`Panos_data/run-2013-09-XX/analysis/hiclib/hiclib.CUTLL1-GSI1-rep1-HiC/filtered_reads.reg+ Panos_data/run-2013-09-XX/analysis/hiclib/hiclib.CUTLL1-GSI1-rep2-HiC/filtered_reads.reg+ Panos_data/run-2013-09-XX/analysis/hiclib/hiclib.CUTLL1-GSI2-rep1-HiC/filtered_reads.reg+ Panos_data/run-2013-09-XX/analysis/hiclib/hiclib.CUTLL1-GSI2-rep2-HiC/filtered_reads.reg+`

**Give me back the files that do not match the pattern (grep -v)**
X2=$(ls -1 `Panos_data/run-*/analysis/hiclib/hiclib.CUTLL1-*/filtered_reads.reg+` | grep -v "GSI.-rep.-HiC/filtered_reads.reg+$")

echo $X2

`Panos_data/run-2012-06-XX/analysis/hiclib/hiclib.CUTLL1-rep1-DMSO-HindIII-HiC/filtered_reads.reg+ Panos_data/run-2012-06-XX/analysis/hiclib/hiclib.CUTLL1-rep1-GSI2h-HindIII-HiC/filtered_reads.reg+ Panos_data/run-2012-06-XX/analysis/hiclib/hiclib.CUTLL1-rep1-GSI-HindIII-HiC/filtered_reads.reg+ Panos_data/run-2012-06-XX/analysis/hiclib/hiclib.CUTLL1-rep2-DMSO-HindIII-HiC/filtered_reads.reg+ Panos_data/run-2012-06-XX/analysis/hiclib/hiclib.CUTLL1-rep2-GSI2h-HindIII-HiC/filtered_reads.reg+ Panos_data/run-2012-06-XX/analysis/hiclib/hiclib.CUTLL1-rep2-GSI6h-HindIII-HiC/filtered_reads.reg+ Panos_data/run-2012-06-XX/analysis/hiclib/hiclib.CUTLL1-rep2-GSI-HindIII-HiC/filtered_reads.reg+ Panos_data/run-2013-09-XX/analysis/hiclib/hiclib.CUTLL1-DMSO-rep1-HiC/filtered_reads.reg+ Panos_data/run-2013-09-XX/analysis/hiclib/hiclib.CUTLL1-DMSO-rep2-HiC/filtered_reads.reg+ Panos_data/run-2013-09-XX/analysis/hiclib/hiclib.CUTLL1-UNTREATED-rep1-HiC/filtered_reads.reg+ Panos_data/run-2013-09-XX/analysis/hiclib/hiclib.CUTLL1-UNTREATED-rep2-HiC/filtered_reads.reg+`

Based on these, the matrices were created.



2. I added functionality to the script `hic_matrix.R` that Aris wrote, in order to create
heatmaps for certain areas. This can be done using a command like the one shown below:

`./Scripts/hic_matrix.R heatmap -o chr8_120651665-136046022_dif_heatmap.pdf -l chr8:120651665-136046022 -r 100000 resolution.100kb/CUTLL.HiC/ resolution.100kb/CUTLL.control.GSI/`

3. I also added a function to `hic_matrix.R` in order to calculate differences between
two input matrices (the script is called `hic_matrix_new_dif.R`). In order to calculate the
difference between two matrices (in the current version the difference is calculated
as the fraction `normalized_matrix1+epsilon/normalized_matrix2+epsilon`) (not log2).

Then estimation is performed as usual:

<!-- Code goes here -->
`qsub ../../Scripts/run_hic_matrix.sh CUTLL.HiC.vs.CUTLL.control.GSI.dif none 10000 100 fused1Dalt_flsa`

A git repository was created with all the scripts of the rotation so far.
It will also be used for the scripts of the PhD in the future. The repository
is on the cluster (Scripts directory in Case1) and its address on the web is: <https://chlazaris@bitbucket.org/chlazaris/cluster_scripts.git>.

**Future plans**  

* Make a clear timeline on the expectations for the project and
what to do, when. 
* Modify the hic_matrix script in order to be able to create
heatmaps for specific chromosome locations based on the .RData
files (the products of the estimation).
* Meet with Iannis and Aris at 12pm on Friday in order to
discuss project so far and future plans.

<!-- END OF DAY -->

# Date: May 22 2014
## Title: Analysing the results of the fold difference (RPKM) for 1MB
## (Panos' data CUTLL.HiC.vs.CUTLL.control.GSI)

<!-- What to do -->
**Observations**

1. It appears that chr7 has a small area that should
be the centromere while chr9 has a large one. Based on a search that
I made though, this does not seem to be the case
as on `http://genome.ucsc.edu/cgi-bin/hgTables` and by searching with
`AllTables:gap` (for hg19), the centromeres for all chromosomes should 
be 3Mbp in size. I am wondering what I am going to see
with the overall heatmap for 10MB resolution. 

I also analysed the results (RPKM normalization and production of heatmaps) for 10MB resolution. The script for doing it (for HiC which is actually DMSO vs. GSI can be found in the `Scripts` directory on the cluster).


<!-- BEGINNING OF DAY -->

# Date: June 2 2014
## Recreating the matrices for Ren 2012 data (mm10 and 100kb)
## and running new versions of R and MATLAB randomization
## scripts that can work for all matrices in a directory

As I accidentally deleted the matrices as well and not only
the estimations (as I should), I recreated the 100kb resolution
matrices for both HindIII and NcoI for the Ren-2012 data.

The matrices are located in `Evaluation/Ren-2012/` and then
there are two subdirectories (one for each enzyme). In those
subdirectories, there are three subsubdirectories: `matrix`,
`random.matrix` and `complete.random.matrix`. The first one
contains the original matrices, the second the randomized
version of the matrices where only the diagonals have been
randomized and the last one, the completely randomized diagonals.

I did this procedure both for HindIII and NcoI and I also 
started creating the 10kb resolution matrices, which will
apparently take a while. 

I also run `hic_matrix.v.1.0` with settings `max-gamma=1000`,
`n-gamma=100` and algorithm `fused1Dsymm_flsa` both for HindIII
and NcoI, so tomorrow I will be able to do the comparisons and
create the corresponding heatmaps.

<!-- END OF DAY -->

### Date: June 17th 2014
### Title: Running fused1Dsymm_flsa and fused2D_flsa successfully
### with 10MB resolution

<!-- What to do -->

1. Run fused1Dsymm_flsa and fused2D_flsa
2. Compare the results
3. Create 5MB matrices and try to repeat
the same there.

Initially there was a problem with my quota
and I was getting an error (Eqw). Stratos solved
this issue (at least for now) by doubling my 
quota. Once the issue was resolved, the randomization
(diagonals only) (`run_randomize_matrix.sh`) and 
complete randomization (`run_compl_randomize.sh`) worked
extremely fast for the 10MB resolution. The estimation
worked fast as well, both for fused1Dsymm_flsa and 
fused2D_flsa. The results seem to be strikingly good
so far, as the objective functions (error) are identical.
I performed the analysis for fraction of the genome matrix
(f-value=0.1) and there is structure there as well (it is 
not that everything looks the same).

I am trying to perform the same procedure for 5MB but will
`fused1Dsymm_flsa` works really fast, `fused2D_flsa` takes
particularly long time to run. Right now I am running on regular
node. If I face any memory issues, I will try to run it on highmem
as well.

I also modified the scripts `run_hic_matrix.sh` and `compare_RData.sh`
in order to use the new updateable `hic_matrix.r` version that I have
git-cloned in my home dir on the cluster. Moreover the new versions
of the scripts include the algorithm name to the directory name with the 
estimations.

*Future directions*

1. Try to run fused2D successfully for finer
resolutions to see if we can substitute it with
fused1Dsymm without issues.
2. Start writing the paper. 

<!-- END OF DAY -->

# Date: June 26th 2014
## Title: Generating Hi-C interaction text files (HiC) and visualizing 
## the interactions on UWash Epigenome Browser.

<!-- What to do -->

1. The scripts `run_Panos_hic.sh` and `create_genome_matrix` were created
for this purpose. The former is calling the latter in order to create
a text file that contains the Hi-C interaction pairs along with the corresponding scores (number of interactions).
2. I solved how to create the heatmap on UWash Epigenome Browser. It seems
that all the interactions are needed and also I have to upload the file
as text but from the `custom tracks` menu.

**Below the first successful Hi-C data representation is shown. I have 
uploaded the sample for CUTLL GSI and for resolution 1MB. The area has been
zoomed in such a way that "domains" are visible.**  

<!-- Non-enumerated list -->
**Future plans**

* There is an issue with speed. It takes forever 
to upload the data. It worked for 1MB resolution
but there was problem with 100kb.
* This will be a problem when Panos gets more reads
as well.
* This probably means that we have to test with the
other format as well. It is probably way more efficient.

<!-- END OF DAY -->

# Date: June 30 2014
## Title: Successful creation of online session with 
## Panos's CUTLL Hi-C data

<!-- What to do -->

1. Created .gz and .gz.tbi (index) files using Aris's new
script that performs conversion and sorting
2. I changed my create_genome_hic script to use Aris's new
script.
3. We created with Eric (Peskin) a directory called epigenome
within aifantislab sequencing results. The directory is password
protected and I put inside all 6 samples from the CUTLL data 
(2 resolutions (100kb and 1000kb), 3 files per resolution).

In order to avoid issues with other labs being able to see the data
we used a very basic type of "cryptography" giving useless names to
the files. Here you can find the key between fake and real names:

**Key**  

* Sample1:   CUTLL.DMSO.1000kb
* Sample2:   CUTLL.GSI.1000kb
* Sample3:   CUTLL.post-GSI.1000kb
* Sample4:   CUTLL.DMSO.100kb
* Sample5:   CUTLL.GSI.100kb
* Sample6:   CUTLL.post-GSI.100kb

The URLs given to the epigenome browser were the following ones:

https://nyuepi:bl00045nyuepi@genome.nyumc.org/results/aifantislab/epigenome/panos-final/sample1.washu.tsv.gz

https://nyuepi:bl00045nyuepi@genome.nyumc.org/results/aifantislab/epigenome/panos-final/sample2.washu.tsv.gz

https://nyuepi:bl00045nyuepi@genome.nyumc.org/results/aifantislab/epigenome/panos-final/sample3.washu.tsv.gz

https://nyuepi:bl00045nyuepi@genome.nyumc.org/results/aifantislab/epigenome/panos-final/sample4.washu.tsv.gz

https://nyuepi:bl00045nyuepi@genome.nyumc.org/results/aifantislab/epigenome/panos-final/sample5.washu.tsv.gz

https://nyuepi:bl00045nyuepi@genome.nyumc.org/results/aifantislab/epigenome/panos-final/sample6.washu.tsv.gz

The session was called "Shrek the ogre" and the link for the
session is: http://epigenomegateway.wustl.edu/browser/?genome=hg19&session=yyat2zctUi&statusId=1868751362





